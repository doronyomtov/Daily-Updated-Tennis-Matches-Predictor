{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55fb25",
   "metadata": {
    "id": "6d55fb25",
    "outputId": "786ba45d-d0c5-40ad-92d9-4bcb4dbb1cb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64781it [00:01, 42041.80it/s]\n",
      "25/03/25 20:52:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/03/25 20:53:05 WARN TaskSetManager: Stage 0 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/03/25 20:53:09 WARN TaskSetManager: Stage 1 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:10 WARN TaskSetManager: Stage 2 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:10 WARN TaskSetManager: Stage 3 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:10 WARN TaskSetManager: Stage 4 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 5 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 6 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 7 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 8 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 9 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 10 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 11 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 12 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:11 WARN TaskSetManager: Stage 13 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:12 WARN TaskSetManager: Stage 14 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:12 WARN TaskSetManager: Stage 15 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:12 WARN TaskSetManager: Stage 16 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:13 WARN TaskSetManager: Stage 17 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:17 WARN TaskSetManager: Stage 19 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:18 WARN TaskSetManager: Stage 22 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:19 WARN TaskSetManager: Stage 23 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:19 WARN TaskSetManager: Stage 24 contains a task of very large size (1064 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:20 WARN TaskSetManager: Stage 26 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:22 WARN TaskSetManager: Stage 28 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:23 WARN TaskSetManager: Stage 30 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:24 WARN TaskSetManager: Stage 32 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:25 WARN TaskSetManager: Stage 34 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:53:27 WARN TaskSetManager: Stage 36 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Job \"reset_var (trigger: cron[hour='20', minute='55'], next run at: 2025-03-26 20:55:00 IST)\" raised an exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/apscheduler/executors/base.py\", line 131, in run_job\n",
      "    retval = job.func(*job.args, **job.kwargs)\n",
      "  File \"/tmp/ipykernel_18347/1773425766.py\", line 285, in reset_var\n",
      "    scheduler.add_job(data_retrieval)\n",
      "NameError: name 'data_retrieval' is not defined\n",
      "64781it [00:01, 34594.77it/s]\n",
      "25/03/25 20:55:13 WARN TaskSetManager: Stage 38 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:13 WARN TaskSetManager: Stage 39 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 40 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 41 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 42 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 43 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 44 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 45 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 46 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 47 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 48 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 49 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 50 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 51 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 52 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 53 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:14 WARN TaskSetManager: Stage 54 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:15 WARN TaskSetManager: Stage 55 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:19 WARN TaskSetManager: Stage 57 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:19 WARN TaskSetManager: Stage 60 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:19 WARN TaskSetManager: Stage 61 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:19 WARN TaskSetManager: Stage 62 contains a task of very large size (1064 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:20 WARN TaskSetManager: Stage 64 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:21 WARN TaskSetManager: Stage 66 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:21 WARN TaskSetManager: Stage 68 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:22 WARN TaskSetManager: Stage 70 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:23 WARN TaskSetManager: Stage 72 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/25 20:55:25 WARN TaskSetManager: Stage 74 contains a task of very large size (1063 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import sys\n",
    "\n",
    "\n",
    "def data_engineering(df):\n",
    "    ##Creating id for each unique player\n",
    "    df['Player_1']=df['Player_1'].str.lower()\n",
    "    df['Player_2']=df['Player_2'].str.lower()\n",
    "    df['Winner']=df['Winner'].str.lower()\n",
    "    all_players = list(df['Player_1'].unique()) + list(df['Player_2'].unique())\n",
    "\n",
    "\n",
    "    unique_players = list(set(all_players))\n",
    "    global player_id_map\n",
    "    player_id_map = {player: i for i, player in enumerate(unique_players)}\n",
    "\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df['Player_1_ID'] = new_df['Player_1'].map(player_id_map)\n",
    "    new_df['Player_2_ID'] = new_df['Player_2'].map(player_id_map)\n",
    "    new_df['Winner_id'] = new_df['Winner'].map(player_id_map)\n",
    "    for index, row in new_df.iterrows():\n",
    "        if row['Winner_id'] == row['Player_1_ID']:\n",
    "            new_df.at[index, 'Label'] = 1\n",
    "            new_df.at[index, 'Loser_id'] = row['Player_2_ID']\n",
    "        if row['Winner_id'] == row['Player_2_ID']:\n",
    "            new_df.at[index, 'Label'] = 0\n",
    "            new_df.at[index, 'Loser_id'] = row['Player_1_ID']\n",
    "\n",
    "\n",
    "    #Creating elo for each player (all of them start with 1500 and the maximum gain from one match is 24\n",
    "     ##(similiar to chess))\n",
    "    global elo_players\n",
    "    elo_players = defaultdict(int)\n",
    "    global all_elo\n",
    "    all_elo = defaultdict(lambda: deque())\n",
    "    df_elo = []\n",
    "    k = 24\n",
    "    for w_id, l_id in zip(new_df['Winner_id'], new_df['Loser_id']):\n",
    "\n",
    "        elo_w = elo_players.get(w_id, 1500)\n",
    "        elo_l = elo_players.get(l_id, 1500)\n",
    "\n",
    "        exp_w = 1/(1+10**((elo_l-elo_w)/400))\n",
    "        exp_l = 1/(1+10**((elo_w-elo_l)/400))\n",
    "\n",
    "        elo_w += k*(1-exp_w)\n",
    "        elo_l += k*(0-exp_l)\n",
    "\n",
    "        df_elo.append(elo_w-elo_l)\n",
    "\n",
    "    # Update\n",
    "        elo_players[w_id] = elo_w\n",
    "        elo_players[l_id] = elo_l\n",
    "\n",
    "        all_elo[w_id].append(elo_w)\n",
    "        all_elo[l_id].append(elo_l)\n",
    "    new_df['Elo_diff'] = df_elo\n",
    "\n",
    "\n",
    "    ##Creating elo for each player based on their prefmormance on the court (similiar to the previous one)\n",
    "    global elo_surfaces\n",
    "    elo_surfaces = defaultdict(lambda: defaultdict(int))\n",
    "    all_elo_surfaces = defaultdict(lambda: defaultdict(lambda: deque()))\n",
    "    df_elo = []\n",
    "\n",
    "    for w_id, l_id, surface in zip(new_df['Winner_id'], new_df['Loser_id'], new_df['Surface']):\n",
    "        elo_w = elo_surfaces[surface].get(w_id, 1500)\n",
    "        elo_l = elo_surfaces[surface].get(l_id, 1500)\n",
    "\n",
    "        exp_w = 1/(1+10**((elo_l-elo_w)/400))\n",
    "        exp_l = 1/(1+10**((elo_w-elo_l)/400))\n",
    "\n",
    "        elo_w += k*(1-exp_w)\n",
    "        elo_l += k*(0-exp_l)\n",
    "        df_elo.append(elo_w-elo_l)\n",
    "\n",
    "    # Update\n",
    "        elo_surfaces[surface][w_id] = elo_w\n",
    "        elo_surfaces[surface][l_id] = elo_l\n",
    "\n",
    "        all_elo_surfaces[surface][w_id].append(elo_w)\n",
    "        all_elo_surfaces[surface][l_id].append(elo_l)\n",
    "\n",
    "        for s in [\"Clay\", \"Grass\", \"Hard\", \"Carpet\"]:\n",
    "            if surface != s:\n",
    "                all_elo_surfaces[s][w_id].append(elo_surfaces[s].get(w_id, 1500))\n",
    "                all_elo_surfaces[s][l_id].append(elo_surfaces[s].get(l_id, 1500))\n",
    "\n",
    "    new_df[\"ELO_SURFACE_DIFF\"] = df_elo\n",
    "    new_df.drop(['Tournament',\"Date\",'Series',\"Court\",'Surface',\"Round\",\"Best of\",'Player_1','Player_2','Winner','Score','Player_1_ID','Player_2_ID','Winner_id','Loser_id','Pts_1',\"Pts_2\"],axis=1,inplace=True)\n",
    "    new_df['Rank_1'] =new_df['Rank_1'].astype(float)\n",
    "    new_df['Rank_2'] =new_df['Rank_2'].astype(float)\n",
    "    new_df['Odd_1'] =new_df['Odd_1'].astype(float)\n",
    "    new_df['Odd_2'] =new_df['Odd_2'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_plot():\n",
    "    nadal = list(all_elo[player_id_map.get('nadal r.')])\n",
    "    novak = list(all_elo[player_id_map.get('djokovic n.')])\n",
    "    federer = list(all_elo[player_id_map.get('federer r.')])\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    for player in all_elo.keys():\n",
    "        plt.plot(list(all_elo[player]), marker='.', linewidth=0.1, markersize=1, linestyle='-', color='black')\n",
    "\n",
    "##מוסיף צבעים לשלושת השחקנים עם האלו הכי גבוה\n",
    "    plt.plot(nadal, marker='.', linewidth=0.5, markersize=1, linestyle='-', color='blue', label=\"Rafa Nadal\")\n",
    "    plt.plot(novak, marker='.', linewidth=0.5, markersize=1, linestyle='-', color='red', label=\"Novak Djokovic\")\n",
    "    plt.plot(federer, marker='.', linewidth=0.5, markersize=1, linestyle='-', color='green', label=\"Roger Federer\")\n",
    "    plt.title(\"Elo Ratings Over Time\")\n",
    "    plt.xlabel(\"Match Number\")\n",
    "    plt.ylabel(\"Elo Rating\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def lr_spark(df):\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Logistic regression model\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "    spark_df = spark.createDataFrame(new_df)\n",
    "    feature_cols = ['Rank_1', 'Rank_2', 'Odd_1', 'Odd_2', 'Elo_diff', 'ELO_SURFACE_DIFF']\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    spark_df = assembler.transform(spark_df)\n",
    "    spark_train, spark_test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='Label')\n",
    "    lr_model = lr.fit(spark_train)\n",
    "    predictions = lr_model.transform(spark_test)\n",
    "    evaluator = MulticlassClassificationEvaluator( labelCol='Label', predictionCol='prediction', metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    return lr_model,accuracy\n",
    "\n",
    "\n",
    "\n",
    "def rf_spark(df):\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Logistic regression model\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "    spark_df = spark.createDataFrame(new_df)\n",
    "    feature_cols = ['Rank_1', 'Rank_2', 'Odd_1', 'Odd_2', 'Elo_diff', 'ELO_SURFACE_DIFF']\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    spark_df = assembler.transform(spark_df)\n",
    "    spark_train, spark_test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    rf = RandomForestClassifier(featuresCol='features', labelCol='Label', numTrees=100, seed=42)\n",
    "    spark_train, spark_test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    rf_model = rf.fit(spark_train)\n",
    "    predictions = rf_model.transform(spark_test)\n",
    "    evaluator = MulticlassClassificationEvaluator( labelCol='Label', predictionCol='prediction', metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    return rf_model,accuracy\n",
    "\n",
    "\n",
    "def get_an_input_for_prediction(text):\n",
    "    user_input = input(text)\n",
    "    return user_input\n",
    "\n",
    "\n",
    "def use_a_model(model):\n",
    "    surfaces =[\"Clay\", \"Grass\", \"Hard\", \"Carpet\"]\n",
    "    surface = None\n",
    "    Player_1 = None\n",
    "    while Player_1 not in player_id_map.keys():\n",
    "        Player_1 = get_an_input_for_prediction(\"What is the Name of Player 1\")\n",
    "        #lets say we get Roger Federer and we need to make it into federer r.\n",
    "        Player_1 =f\"{Player_1.split()[1].lower()} {Player_1.split()[0][0].lower()}.\"\n",
    "        if Player_1 not in player_id_map.keys():\n",
    "            print(\"Player not found try again\")\n",
    "    Player_2 = None\n",
    "    while Player_2 not in player_id_map.keys() or Player_1 == Player_2:\n",
    "        Player_2 = get_an_input_for_prediction(\"What is the Name of Player 2\")\n",
    "        #lets say we get Roger Federer and we need to make it into federer r.\n",
    "        Player_2 =f\"{Player_2.split()[1].lower()} {Player_2.split()[0][0].lower()}.\"\n",
    "\n",
    "        if Player_2 not in player_id_map.keys():\n",
    "            print(\"Player not found try again\")\n",
    "        if Player_2 == Player_1:\n",
    "            print('Player cannot play himself')\n",
    "    while surface not in surfaces:\n",
    "        surface = get_an_input_for_prediction(\"What is the surface? (Clay,Grass,Hard,Carpet)\")\n",
    "        if surface not in surfaces:\n",
    "            print('Surface is invalid try again')\n",
    "\n",
    "    Rank_1 = int(get_an_input_for_prediction(\"What is player 1 ranking?\"))\n",
    "    Rank_2 = int(get_an_input_for_prediction(\"What is player 2 ranking\"))\n",
    "    Odd_1 = float(get_an_input_for_prediction(\"What is player 1 odds?\"))\n",
    "    Odd_2 = float(get_an_input_for_prediction(\"What is player 2 odds?\"))\n",
    "    Player_1_ID= player_id_map.get(Player_1)\n",
    "    Player_2_ID= player_id_map.get(Player_2)\n",
    "    Elo_diff = float(elo_players.get(Player_1_ID)-elo_players.get(Player_2_ID))\n",
    "    ELO_SURFACE_DIFF=float(elo_surfaces.get(surface).get(Player_1_ID)-elo_surfaces.get(surface).get(Player_2_ID))\n",
    "    data = [(Rank_1, Rank_2, Odd_1, Odd_2, Elo_diff, ELO_SURFACE_DIFF)]\n",
    "    columns = [\"Rank_1\", \"Rank_2\", \"Odd_1\", \"Odd_2\", \"Elo_diff\", \"ELO_SURFACE_DIFF\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "    df_transformed = assembler.transform(df)\n",
    "    prediction = model.transform(df_transformed)\n",
    "    if prediction.select('prediction').take(1)[0][0] == 1:\n",
    "        print(f'Model predict Player 1 to win')\n",
    "    else:\n",
    "        print(f'Model predict Player 2 to win')\n",
    "\n",
    "\n",
    "\n",
    "def data_retrivel():\n",
    "    consumer = KafkaConsumer(\n",
    "        'tennis_daily_data',\n",
    "        bootstrap_servers='localhost:9092',\n",
    "        group_id=str(uuid.uuid4()),\n",
    "        auto_offset_reset='earliest',\n",
    "        enable_auto_commit=True,\n",
    "        value_deserializer=lambda m: m.decode('utf-8')\n",
    "    )\n",
    "\n",
    "# קריאת הנתונים\n",
    "    data = []\n",
    "    x=0\n",
    "    for message in tqdm(consumer):\n",
    "        if x>1 and data[0]==message.value.split(','):\n",
    "            break\n",
    "        x+=1\n",
    "        row = message.value.split(',')  # פיצול לפי פסיקים\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data,columns=['Tournament', 'Date', 'Series', 'Court', 'Surface', 'Round', 'Best of', 'Player_1', 'Player_2', 'Winner', 'Rank_1', 'Rank_2', 'Pts_1', 'Pts_2', 'Odd_1', 'Odd_2', 'Score']\n",
    ")\n",
    "    global new_df\n",
    "    new_df = data_engineering(df)\n",
    "    #lr_sklearn,acc_lr_sklearn = lr_sklearn(new_df)\n",
    "    lr_spark_model,acc_lr_spark = lr_spark(new_df)\n",
    "   # rf_sklearn,acc_rf_sklearn = random_forest(new_df)\n",
    "    rf_spark_model,acc_rf_spark = rf_spark(new_df)\n",
    "    scheduler.add_job(reset_var, 'cron', hour=8, minute=0)\n",
    "    ui(acc_lr_spark,acc_rf_spark,lr_spark_model,rf_spark_model)\n",
    "    \n",
    "def ui(acc_lr_spark,acc_rf_spark,lr_spark_model,rf_spark_model):\n",
    "    global user_input\n",
    "    user_input = None\n",
    "    while True and user_input !=9:\n",
    "        user_input = input(f'Input 1 To see Elo Graph\\n Input 2 To see Models accuracy\\n Input 3 To Predict Match on a model\\n')\n",
    "        if int(user_input) == 1:\n",
    "            create_plot()\n",
    "        if int(user_input) == 2:\n",
    "            print(f'The accuracy for Logistic Regression is {acc_lr_spark}\\n The accuracy for Random Forest is {acc_rf_spark}')\n",
    "        if int(user_input) == 3:\n",
    "            user_input =input(f'Input 1 To use Logistic Regression\\n Input 2 To use Random Forest\\n' )\n",
    "            if int(user_input) ==1:\n",
    "                use_a_model(lr_spark_model)\n",
    "            if int(user_input) ==2:\n",
    "                use_a_model(rf_spark_model)\n",
    "        if int(user_input) == 9:\n",
    "            return None\n",
    "\n",
    "def reset_var():\n",
    "    user_input =9\n",
    "    scheduler.add_job(data_retrieval)\n",
    "        \n",
    "\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.add_job(data_retrivel, 'cron', hour=8, minute=0)\n",
    "scheduler.start()\n",
    "data_retrivel()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a073ffc0",
   "metadata": {
    "id": "a073ffc0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a068fa",
   "metadata": {
    "id": "98a068fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe185b1f",
   "metadata": {
    "id": "fe185b1f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fd4ad",
   "metadata": {
    "id": "b72fd4ad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
