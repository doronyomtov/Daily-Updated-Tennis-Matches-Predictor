{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44981d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_174493/3917353308.py\", line 302, in schedule_data_retrieval\n",
      "  File \"/home/linuxu/anaconda3/lib/python3.9/site-packages/schedule/__init__.py\", line 514, in at\n",
      "    raise ScheduleValueError(\n",
      "schedule.ScheduleValueError: Invalid time format for a daily job (valid format is HH:MM(:SS)?)\n",
      "64781it [00:01, 46229.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in data retrieval: name 'new_df' is not defined\n",
      "{'new_df':        Rank_1  Rank_2  Odd_1  Odd_2  Label    Elo_diff  ELO_SURFACE_DIFF\n",
      "0        63.0    77.0  -1.00  -1.00    1.0   24.000000         24.000000\n",
      "1        56.0     5.0  -1.00  -1.00    0.0   24.000000         24.000000\n",
      "2        40.0   655.0  -1.00  -1.00    1.0   24.000000         24.000000\n",
      "3        87.0    65.0  -1.00  -1.00    0.0   24.000000         24.000000\n",
      "4        81.0   198.0  -1.00  -1.00    1.0   24.000000         24.000000\n",
      "...       ...     ...    ...    ...    ...         ...               ...\n",
      "64776    14.0    12.0   1.44   2.75    1.0  158.527290        131.938448\n",
      "64777    26.0     3.0  10.00   1.06    0.0  312.206606        333.226833\n",
      "64778    13.0     6.0   2.50   1.53    1.0  -27.161527        -44.764316\n",
      "64779     3.0    14.0   1.30   3.50    0.0  -76.428717        -41.946634\n",
      "64780    14.0    13.0   1.57   2.38    1.0   93.018432         79.630298\n",
      "\n",
      "[64781 rows x 7 columns], 'lr_spark_model': None, 'rf_spark_model': None, 'acc_lr_spark': None, 'acc_rf_spark': None, 'retrieval_in_progress': True}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import sys\n",
    "import schedule\n",
    "import time\n",
    "import threading\n",
    "import uuid\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "\n",
    "def data_engineering(df):\n",
    "    ##Creating id for each unique player\n",
    "    df['Player_1']=df['Player_1'].str.lower()\n",
    "    df['Player_2']=df['Player_2'].str.lower()\n",
    "    df['Winner']=df['Winner'].str.lower()\n",
    "    all_players = list(df['Player_1'].unique()) + list(df['Player_2'].unique())\n",
    "\n",
    "\n",
    "    unique_players = list(set(all_players))\n",
    "    global player_id_map\n",
    "    player_id_map = {player: i for i, player in enumerate(unique_players)}\n",
    "\n",
    "\n",
    "    new_df = df.copy()\n",
    "    new_df['Player_1_ID'] = new_df['Player_1'].map(player_id_map)\n",
    "    new_df['Player_2_ID'] = new_df['Player_2'].map(player_id_map)\n",
    "    new_df['Winner_id'] = new_df['Winner'].map(player_id_map)\n",
    "    for index, row in new_df.iterrows():\n",
    "        if row['Winner_id'] == row['Player_1_ID']:\n",
    "            new_df.at[index, 'Label'] = 1\n",
    "            new_df.at[index, 'Loser_id'] = row['Player_2_ID']\n",
    "        if row['Winner_id'] == row['Player_2_ID']:\n",
    "            new_df.at[index, 'Label'] = 0\n",
    "            new_df.at[index, 'Loser_id'] = row['Player_1_ID']\n",
    "\n",
    "\n",
    "    #Creating elo for each player (all of them start with 1500 and the maximum gain from one match is 24\n",
    "     ##(similiar to chess))\n",
    "    global elo_players\n",
    "    elo_players = defaultdict(int)\n",
    "    global all_elo\n",
    "    all_elo = defaultdict(lambda: deque())\n",
    "    df_elo = []\n",
    "    k = 24\n",
    "    for w_id, l_id in zip(new_df['Winner_id'], new_df['Loser_id']):\n",
    "\n",
    "        elo_w = elo_players.get(w_id, 1500)\n",
    "        elo_l = elo_players.get(l_id, 1500)\n",
    "\n",
    "        exp_w = 1/(1+10**((elo_l-elo_w)/400))\n",
    "        exp_l = 1/(1+10**((elo_w-elo_l)/400))\n",
    "\n",
    "        elo_w += k*(1-exp_w)\n",
    "        elo_l += k*(0-exp_l)\n",
    "\n",
    "        df_elo.append(elo_w-elo_l)\n",
    "\n",
    "    # Update\n",
    "        elo_players[w_id] = elo_w\n",
    "        elo_players[l_id] = elo_l\n",
    "\n",
    "        all_elo[w_id].append(elo_w)\n",
    "        all_elo[l_id].append(elo_l)\n",
    "    new_df['Elo_diff'] = df_elo\n",
    "\n",
    "\n",
    "    ##Creating elo for each player based on their prefmormance on the court (similiar to the previous one)\n",
    "    global elo_surfaces\n",
    "    elo_surfaces = defaultdict(lambda: defaultdict(int))\n",
    "    all_elo_surfaces = defaultdict(lambda: defaultdict(lambda: deque()))\n",
    "    df_elo = []\n",
    "\n",
    "    for w_id, l_id, surface in zip(new_df['Winner_id'], new_df['Loser_id'], new_df['Surface']):\n",
    "        elo_w = elo_surfaces[surface].get(w_id, 1500)\n",
    "        elo_l = elo_surfaces[surface].get(l_id, 1500)\n",
    "\n",
    "        exp_w = 1/(1+10**((elo_l-elo_w)/400))\n",
    "        exp_l = 1/(1+10**((elo_w-elo_l)/400))\n",
    "\n",
    "        elo_w += k*(1-exp_w)\n",
    "        elo_l += k*(0-exp_l)\n",
    "        df_elo.append(elo_w-elo_l)\n",
    "\n",
    "    # Update\n",
    "        elo_surfaces[surface][w_id] = elo_w\n",
    "        elo_surfaces[surface][l_id] = elo_l\n",
    "\n",
    "        all_elo_surfaces[surface][w_id].append(elo_w)\n",
    "        all_elo_surfaces[surface][l_id].append(elo_l)\n",
    "\n",
    "        for s in [\"Clay\", \"Grass\", \"Hard\", \"Carpet\"]:\n",
    "            if surface != s:\n",
    "                all_elo_surfaces[s][w_id].append(elo_surfaces[s].get(w_id, 1500))\n",
    "                all_elo_surfaces[s][l_id].append(elo_surfaces[s].get(l_id, 1500))\n",
    "\n",
    "    new_df[\"ELO_SURFACE_DIFF\"] = df_elo\n",
    "    new_df.drop(['Tournament',\"Date\",'Series',\"Court\",'Surface',\"Round\",\"Best of\",'Player_1','Player_2','Winner','Score','Player_1_ID','Player_2_ID','Winner_id','Loser_id','Pts_1',\"Pts_2\"],axis=1,inplace=True)\n",
    "    new_df['Rank_1'] =new_df['Rank_1'].astype(float)\n",
    "    new_df['Rank_2'] =new_df['Rank_2'].astype(float)\n",
    "    new_df['Odd_1'] =new_df['Odd_1'].astype(float)\n",
    "    new_df['Odd_2'] =new_df['Odd_2'].astype(float)\n",
    "\n",
    "\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_plot():\n",
    "    nadal = list(all_elo[player_id_map.get('nadal r.')])\n",
    "    novak = list(all_elo[player_id_map.get('djokovic n.')])\n",
    "    federer = list(all_elo[player_id_map.get('federer r.')])\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    for player in all_elo.keys():\n",
    "        plt.plot(list(all_elo[player]), marker='.', linewidth=0.1, markersize=1, linestyle='-', color='black')\n",
    "\n",
    "##מוסיף צבעים לשלושת השחקנים עם האלו הכי גבוה\n",
    "    plt.plot(nadal, marker='.', linewidth=0.5, markersize=1, linestyle='-', color='blue', label=\"Rafa Nadal\")\n",
    "    plt.plot(novak, marker='.', linewidth=0.5, markersize=1, linestyle='-', color='red', label=\"Novak Djokovic\")\n",
    "    plt.plot(federer, marker='.', linewidth=0.5, markersize=1, linestyle='-', color='green', label=\"Roger Federer\")\n",
    "    plt.title(\"Elo Ratings Over Time\")\n",
    "    plt.xlabel(\"Match Number\")\n",
    "    plt.ylabel(\"Elo Rating\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.draw()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def lr_spark(df):\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Logistic regression model\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    feature_cols = ['Rank_1', 'Rank_2', 'Odd_1', 'Odd_2', 'Elo_diff', 'ELO_SURFACE_DIFF']\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    spark_df = assembler.transform(spark_df)\n",
    "    spark_train, spark_test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='Label')\n",
    "    lr_model = lr.fit(spark_train)\n",
    "    predictions = lr_model.transform(spark_test)\n",
    "    evaluator = MulticlassClassificationEvaluator( labelCol='Label', predictionCol='prediction', metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    return lr_model,accuracy\n",
    "\n",
    "\n",
    "\n",
    "def rf_spark(df):\n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Logistic regression model\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    feature_cols = ['Rank_1', 'Rank_2', 'Odd_1', 'Odd_2', 'Elo_diff', 'ELO_SURFACE_DIFF']\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    spark_df = assembler.transform(spark_df)\n",
    "    spark_train, spark_test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    rf = RandomForestClassifier(featuresCol='features', labelCol='Label', numTrees=100, seed=42)\n",
    "    spark_train, spark_test = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    rf_model = rf.fit(spark_train)\n",
    "    predictions = rf_model.transform(spark_test)\n",
    "    evaluator = MulticlassClassificationEvaluator( labelCol='Label', predictionCol='prediction', metricName='accuracy')\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    return rf_model,accuracy\n",
    "\n",
    "\n",
    "def get_an_input_for_prediction(text):\n",
    "    user_input = input(text)\n",
    "    return user_input\n",
    "\n",
    "\n",
    "def use_a_model(model):\n",
    "    surfaces =[\"Clay\", \"Grass\", \"Hard\", \"Carpet\"]\n",
    "    surface = None\n",
    "    Player_1 = None\n",
    "    while Player_1 not in player_id_map.keys():\n",
    "        Player_1 = get_an_input_for_prediction(\"What is the Name of Player 1\")\n",
    "        #lets say we get Roger Federer and we need to make it into federer r.\n",
    "        Player_1 =f\"{Player_1.split()[1].lower()} {Player_1.split()[0][0].lower()}.\"\n",
    "        if Player_1 not in player_id_map.keys():\n",
    "            print(\"Player not found try again\")\n",
    "    Player_2 = None\n",
    "    while Player_2 not in player_id_map.keys() or Player_1 == Player_2:\n",
    "        Player_2 = get_an_input_for_prediction(\"What is the Name of Player 2\")\n",
    "        #lets say we get Roger Federer and we need to make it into federer r.\n",
    "        Player_2 =f\"{Player_2.split()[1].lower()} {Player_2.split()[0][0].lower()}.\"\n",
    "\n",
    "        if Player_2 not in player_id_map.keys():\n",
    "            print(\"Player not found try again\")\n",
    "        if Player_2 == Player_1:\n",
    "            print('Player cannot play himself')\n",
    "    while surface not in surfaces:\n",
    "        surface = get_an_input_for_prediction(\"What is the surface? (Clay,Grass,Hard,Carpet)\")\n",
    "        if surface not in surfaces:\n",
    "            print('Surface is invalid try again')\n",
    "\n",
    "    Rank_1 = int(get_an_input_for_prediction(\"What is player 1 ranking?\"))\n",
    "    Rank_2 = int(get_an_input_for_prediction(\"What is player 2 ranking\"))\n",
    "    Odd_1 = float(get_an_input_for_prediction(\"What is player 1 odds?\"))\n",
    "    Odd_2 = float(get_an_input_for_prediction(\"What is player 2 odds?\"))\n",
    "    Player_1_ID= player_id_map.get(Player_1)\n",
    "    Player_2_ID= player_id_map.get(Player_2)\n",
    "    Elo_diff = float(elo_players.get(Player_1_ID)-elo_players.get(Player_2_ID))\n",
    "    ELO_SURFACE_DIFF=float(elo_surfaces.get(surface).get(Player_1_ID)-elo_surfaces.get(surface).get(Player_2_ID))\n",
    "    data = [(Rank_1, Rank_2, Odd_1, Odd_2, Elo_diff, ELO_SURFACE_DIFF)]\n",
    "    columns = [\"Rank_1\", \"Rank_2\", \"Odd_1\", \"Odd_2\", \"Elo_diff\", \"ELO_SURFACE_DIFF\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "    df_transformed = assembler.transform(df)\n",
    "    prediction = model.transform(df_transformed)\n",
    "    if prediction.select('prediction').take(1)[0][0] == 1:\n",
    "        print(f'Model predict Player 1 to win')\n",
    "    else:\n",
    "        print(f'Model predict Player 2 to win')\n",
    "\n",
    "\n",
    "\n",
    "# Event to coordinate between threads\n",
    "data_ready_event = threading.Event()\n",
    "stop_event = threading.Event()\n",
    "\n",
    "def data_retrivel():\n",
    "    \"\"\"\n",
    "    Perform data retrieval and model training\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set flag that retrieval is in progress\n",
    "        global_data['retrieval_in_progress'] = True\n",
    "        \n",
    "        consumer = KafkaConsumer(\n",
    "            'tennis_daily_data',\n",
    "            bootstrap_servers='localhost:9092',\n",
    "            group_id=str(uuid.uuid4()),\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            value_deserializer=lambda m: m.decode('utf-8')\n",
    "        )\n",
    "        \n",
    "        # קריאת הנתונים\n",
    "        data = []\n",
    "        x = 0\n",
    "        for message in tqdm(consumer):\n",
    "            if x > 1 and data[0] == message.value.split(','):\n",
    "                break\n",
    "            x += 1\n",
    "            row = message.value.split(',')  # פיצול לפי פסיקים\n",
    "            data.append(row)\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=['Tournament', 'Date', 'Series', 'Court', 'Surface', 'Round', 'Best of', 'Player_1', 'Player_2', 'Winner', 'Rank_1', 'Rank_2', 'Pts_1', 'Pts_2', 'Odd_1', 'Odd_2', 'Score'])     \n",
    "        global_data['new_df'] = data_engineering(df)        \n",
    "        global_data['lr_spark_model'], global_data['acc_lr_spark'] = lr_spark(global_data['new_df'])\n",
    "        global_data['rf_spark_model'], global_data['acc_rf_spark'] = rf_spark(global_data['new_df'])        \n",
    "        # Signal that data is ready\n",
    "        data_ready_event.set()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in data retrieval: {e}\")\n",
    "        print(global_data)\n",
    "    finally:\n",
    "        # Reset the retrieval in progress flag\n",
    "        global_data['retrieval_in_progress'] = False\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "def schedule_data_retrieval():\n",
    "    \"\"\"\n",
    "    Schedule daily data retrieval at 8 AM\n",
    "    \"\"\"\n",
    "    schedule.every().day.at(\"20:0\").do(data_retrivel)\n",
    "    \n",
    "    while not stop_event.is_set():\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "\n",
    "def ui():\n",
    "    \"\"\"\n",
    "    Interactive UI function with handling for background data retrieval\n",
    "    \"\"\"\n",
    "    while not stop_event.is_set():\n",
    "        # Wait if a retrieval is in progress\n",
    "        while global_data['retrieval_in_progress']:\n",
    "            print(\"Data retrieval in progress. Please wait...\")\n",
    "            data_ready_event.wait()\n",
    "            data_ready_event.clear()\n",
    "        \n",
    "        try:\n",
    "            user_input = input(f'Input 1 To see Elo Graph\\n Input 2 To see Models accuracy\\n Input 3 To Predict Match on a model\\n Input 9 To Exit\\n')\n",
    "            user_input = int(user_input)\n",
    "            \n",
    "            if user_input == 1:\n",
    "                create_plot()\n",
    "            elif user_input == 2:\n",
    "                if global_data['lr_spark_model'] and global_data['rf_spark_model']:\n",
    "                    print(f'The accuracy for Logistic Regression is {global_data[\"acc_lr_spark\"]}\\n The accuracy for Random Forest is {global_data[\"acc_rf_spark\"]}')\n",
    "                else:\n",
    "                    print(\"No models trained yet. Run data retrieval first.\")\n",
    "            elif user_input == 3:\n",
    "                if not global_data['lr_spark_model'] or not global_data['rf_spark_model']:\n",
    "                    print(\"No models available. Run data retrieval first.\")\n",
    "                    continue\n",
    "                \n",
    "                model_choice = int(input(f'Input 1 To use Logistic Regression\\n Input 2 To use Random Forest\\n'))\n",
    "                \n",
    "                if model_choice == 1:\n",
    "                    use_a_model(global_data['lr_spark_model'])\n",
    "                elif model_choice == 2:\n",
    "                    use_a_model(global_data['rf_spark_model'])\n",
    "            elif user_input == 9:\n",
    "                stop_event.set()\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid input. Try again.\")\n",
    "        \n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to coordinate threads and run the application\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Scheduler thread\n",
    "        scheduler_thread = threading.Thread(target=schedule_data_retrieval, daemon=True)\n",
    "        scheduler_thread.start()\n",
    "        \n",
    "        # Trigger initial data retrieval\n",
    "        data_retrivel()\n",
    "        \n",
    "        # Run UI\n",
    "        ui()\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nApplication interrupted by user.\")\n",
    "    finally:\n",
    "        # Ensure all threads are stopped\n",
    "        stop_event.set()\n",
    "        sys.exit(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    global_data = {\n",
    "    'new_df': None,\n",
    "    'lr_spark_model': None,\n",
    "    'rf_spark_model': None,\n",
    "    'acc_lr_spark': None,\n",
    "    'acc_rf_spark': None,\n",
    "    'retrieval_in_progress': False\n",
    "    }\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf9763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd022313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
